{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find the sessions that include the region of interest\n",
    "Note that the function list any session that includes **any** of the regions. The funciton to list the sessions that include **all** of the regions will be added in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to https://openalyx.internationalbrainlab.org as user \"intbrainlab\"\n",
      "Total number of possible probe insertions: 73\n",
      "PID-EID pairs saved to data/pid_eids_V1.json\n",
      "the path to the data is /workspaces/onedrive1/文档/ibl_data\n",
      "already done 10\n"
     ]
    }
   ],
   "source": [
    "from functions import  get_epoch_StimOn, get_pid_eid_pairs\n",
    "from functions import monitor_job_status\n",
    "import submitit\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import json \n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(str(Path(os.getcwd()).resolve().parent)) # add the root of the project to the python path\n",
    "# Now import the paths from config.py\n",
    "from config import paths\n",
    "\n",
    "# create the pid_eid_pairs\n",
    "pid_eid_pairs = get_pid_eid_pairs(output_file='data/pid_eids_V1.json', only_passive= False, regions= ['VISp'])\n",
    "\n",
    "# the path to the data (based on config file) and the number of downloaded files\n",
    "print(f'the path to the data is {paths[\"LFP\"]}')\n",
    "already_done = [p[0] for p in pid_eid_pairs if p[0] in [f.split('_')[0] for f in os.listdir(paths['LFP'])]]\n",
    "print(f'already done {len(already_done)}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 parallel submition using submitit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitit\n",
    "import os\n",
    "import json\n",
    "\n",
    "def submit_extract(pid_eid_pair):\n",
    "    pid, eid = pid_eid_pair\n",
    "    get_epoch_StimOn(pid, modee = 'download', overwrite= False, window_secs=[-1, 1.5], save = True, ephys_path = paths['LFP'])\n",
    "\n",
    "\n",
    "already_done_pids = [f.split('_')[0] for f in os.listdir(paths['LFP']) if f.endswith('.npy') ]\n",
    "pid_eid_pair = [p for p in pid_eid_pairs if p[0] not in already_done_pids]\n",
    "# Create an executor\n",
    "executor = submitit.AutoExecutor(folder='logs')\n",
    "executor.update_parameters(slurm_array_parallelism= 50 , mem_gb=10, timeout_min= 120, slurm_partition= 'CPU',  cpus_per_task= 1)\n",
    "# Submit all jobs using map_array                                                                                              \n",
    "all_jobs = executor.map_array(submit_extract, pid_eid_pair)\n",
    "monitor_job_status(all_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 single submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_done_pids = [f.split('_')[0] for f in os.listdir(paths['LFP']) if f.endswith('.npy') ]\n",
    "pid_eid_pair = [p for p in pid_eid_pairs if p[0] not in already_done_pids]\n",
    "for i, pid in enumerate(pid_eid_pair[0]):\n",
    "    print(pid)\n",
    "    print(f'{i}/{len(pid_eid_pair)}')\n",
    "    get_epoch_StimOn(pid, modee = 'download', overwrite= False, window_secs=[-1, 1.5], save = True, ephys_path = paths['LFP'])\n",
    "    break\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  NPY to MNE (optional)\n",
    "due to RAM limitation on local machine, I saved the data in npy format. you can change to fif in this way :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to https://openalyx.internationalbrainlab.org as user \"intbrainlab\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/iblenvlocal/lib/python3.10/site-packages/one/util.py:543: ALFWarning: Multiple revisions: \"\", \"2024-05-06\"\n",
      "  warnings.warn(f'Multiple revisions: {rev_list}', alferr.ALFWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding metadata with 15 columns\n",
      "740 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1667/996204942.py:19: RuntimeWarning: This filename (/workspaces/onedrive1/文档/ibl_data/a8a59fc3-a658-4db4-b5e8-09f1e4df03fd_epoch_stimOn.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  epochs_mne.save(file_path_epochs, overwrite=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Writing channel names to FIF truncated to 15 characters with remapping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PosixPath('/workspaces/onedrive1/文档/ibl_data/a8a59fc3-a658-4db4-b5e8-09f1e4df03fd_epoch_stimOn.fif')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from one.api import ONE\n",
    "import os\n",
    "from functions import get_behavior, get_channels, numpyToMNE\n",
    "# One-time setup for ONE with cache mode set to 'remote'\n",
    "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', cache_dir = None,  silent=True)\n",
    "one = ONE(password='international')\n",
    "# load data\n",
    "pid = 'd1a4e8b1-2b6b-4f0d-8b1d-0b3e6f8e8f1d'\n",
    "epochs_data_np = get_epoch_StimOn(pid, modee = 'download', overwrite= False, window_secs=[-1, 1.5], save = True, ephys_path = paths['LFP'])\n",
    "\n",
    "# convert npy to mne\n",
    "\n",
    "eid, label = one.pid2eid(pid)\n",
    "sfreq =500\n",
    "behavior = get_behavior(eid, modee='download')\n",
    "channels = get_channels(eid,pid, modee = 'download')\n",
    "\n",
    "\n",
    "acronyms = channels['acronym'].tolist()\n",
    "ch_names=[f'{ch}_{i}' for i, ch in enumerate(acronyms)]\n",
    "epochs_mne = numpyToMNE(epochs_data_np, behavior, ch_names, sfreq = sfreq )\n",
    "del epochs_data_np\n",
    "ephys_path = paths['LFP']\n",
    "file_path_epochs = os.path.join(ephys_path, f'{pid}_epoch_stimOn.fif')\n",
    "epochs_mne.save(file_path_epochs, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenvlocal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
